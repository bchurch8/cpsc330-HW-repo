{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 6: Putting it all together \n",
    "### Associated lectures: All material till lecture 13 \n",
    "\n",
    "**Due date: Wednesday, March 15, 2023 at 11:59pm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Submission instructions](#si)\n",
    "- [Understanding the problem](#1)\n",
    "- [Data splitting](#2)\n",
    "- [EDA](#3)\n",
    "- (Optional) [Feature engineering](#4)\n",
    "- [Preprocessing and transformations](#5)\n",
    "- [Baseline model](#6)\n",
    "- [Linear models](#7)\n",
    "- [Different classifiers](#8)\n",
    "- (Optional) [Feature selection](#9)\n",
    "- [Hyperparameter optimization](#10)\n",
    "- [Interpretation and feature importances](#11)\n",
    "- [Results on the test set](#12)\n",
    "- (Optional) [Explaining predictions](#13)\n",
    "- [Summary of the results](#14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2022W2/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work on this homework in a group and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 3. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "At this point we are at the end of supervised machine learning part of the course. So in this homework, you will be working on an open-ended mini-project, where you will put all the different things you have learned so far together to solve an interesting problem.\n",
    "\n",
    "A few notes and tips when you work on this mini-project: \n",
    "\n",
    "#### Tips\n",
    "\n",
    "1. This mini-project is open-ended, and while working on it, there might be some situations where you'll have to use your own judgment and make your own decisions (as you would be doing when you work as a data scientist). **Make sure you explain your decisions whenever necessary.** \n",
    "2. **Do not include everything you ever tried in your submission** -- it's fine just to have your final code. That said, your code should be reproducible and well-documented. For example, if you chose your hyperparameters based on some hyperparameter optimization experiment, you should leave in the code for that experiment so that someone else could re-run it and obtain the same hyperparameters, rather than mysteriously just setting the hyperparameters to some (carefully chosen) values in your code. \n",
    "3. If you realize that you are repeating a lot of code try to organize it in functions. Clear presentation of your code, experiments, and results is the key to be successful in this lab. You may use code from lecture notes or previous lab solutions with appropriate attributions. \n",
    "4. If you are having trouble running models on your laptop because of the size of the dataset, you can create your train/test split in such a way that you have less data in the train split. If you end up doing this, please write a note to the grader in the submission explaining why you are doing it.  \n",
    "\n",
    "#### Assessment\n",
    "\n",
    "We plan to grade fairly and leniently. We don't have some secret target score that you need to achieve to get a good grade. **You'll be assessed on demonstration of mastery of course topics, clear presentation, and the quality of your analysis and results.** For example, if you just have a bunch of code and no text or figures, that's not good. If you do a bunch of sane things and get a lower accuracy than your friend, don't sweat it.\n",
    "\n",
    "#### A final note\n",
    "\n",
    "Finally, this style of this \"project\" question is different from other assignments. It'll be up to you to decide when you're \"done\" -- in fact, this is one of the hardest parts of real projects. But please don't spend WAY too much time on this... perhaps \"a few hours\" (2-8 hours???) is a good guideline for a typical submission. Of course if you're having fun you're welcome to spend as much time as you want! But, if so, try not to do it out of perfectionism or getting the best possible grade. Do it because you're learning and enjoying it. Students from the past cohorts have found such kind of labs useful and fun and I hope you enjoy it as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the problem <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:4}\n",
    "\n",
    "In this mini project, you will be working on a classification problem of predicting whether a customer will cancel the reservation they have made at a hotel. \n",
    "For this problem, you will use [Reservation Cancellation Prediction Dataset](https://www.kaggle.com/datasets/gauravduttakiit/reservation-cancellation-prediction?select=train__dataset.csv). In this data set, there are about 18.000 examples and 18 features (including the target), and the goal is to estimate whether a person will cancel their booking; this column is labeled \"booking_status\" in the data (1 = canceled). \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Spend some time understanding the problem and what each feature means. You can find this information in the documentation on [the dataset page on Kaggle](https://www.kaggle.com/datasets/gauravduttakiit/reservation-cancellation-prediction?select=train__dataset.csv). Write a few sentences on your initial thoughts on the problem and the dataset. \n",
    "2. Download the dataset and read it as a pandas dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "Looking at the problem and dataset, my initial thoughts are that there are going to broadly be two types of cancellations: predictable and un-predictable. A predictable cancellation is one that the data in the dataset could potentially help predict. For example, if the customer is travelling alone, booked shortly before the arrival date, and is only staying a few nights it may be predictable that they cancel. This person could be travelling for business and have plans change often and as such cancel hotels fairly routinely as their travel plans change. On the other hand, if the customer is travelling with children or a large group, booked far in advance, and has made several special requests for their room they might be less likely to cancel as this is probably a vacation or larger trip that was planned well in advance, which often isn't cancelled once booked. However, the data will never be able to help \"un-predictable\" cancellations as these are caused by factors completely irrelevant to the booking itself. For example, even though a family may have booked a vacation many months in advance, they may still have to cancel last-minute because of a health emergency or something similar.  As such, it seems like there will always be a limit to how accurate our model can be because it will not be able to predict these external factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./res_cancel.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data splitting <a name=\"2\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into train and test portions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA <a name=\"3\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Perform exploratory data analysis on the train set.\n",
    "2. Include at least two summary statistics and two visualizations that you find useful, and accompany each one with a sentence explaining it.\n",
    "3. Summarize your initial observations about the data. \n",
    "4. Pick appropriate metric/metrics for assessment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above summary information, we can see that there are no missing entries therefore we will not have to impute any features. Additionally, all of the datatypes are either integer or float but looking at the description of some classes (such as type_of_meal_plan), it looks like some of these integer features actually represent categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancel = train_df.query(\"booking_status == 1\")\n",
    "not_cancel = train_df.query(\"booking_status == 0\")\n",
    "\n",
    "cancel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_cancel = cancel.shape[0] / train_df.shape[0] * 100\n",
    "perc_cancel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 32.6% of the total reservations in the train data set cancelled. This statistic tells us that there is a moderate class imbalance between our two target classes and this problem could be considered a spotting problem (albeit with less class imbalance than other examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = [\"lead_time\", \"avg_price_per_room\"]\n",
    "num_bins = 30\n",
    "\n",
    "#Taken from HW3 implementation\n",
    "for col in numeric_feats:\n",
    "    axes = cancel[[col]].hist(bins = num_bins, alpha = 0.5, label=\"Cancelled\")[0][0]\n",
    "    axes.hist(not_cancel[[col]], bins = num_bins, alpha = 0.5, label=\"Not-Cancel\")\n",
    "\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.title(\"Histogram of \" + col + \" by Cancellation\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above histograms of numeric features we can see that lead_time will most likely be a useful feature as there are many more non-cancellations for low lead times (~ < 150), but more cancellations than non-cancellations for longer lead times (~ > 150). The average room price histogram shows that the distributions are fairly similar, but there are fewer cancellations for rooms less than $100 and slightly more cancellations above $100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feats = [\"no_of_adults\",\"no_of_children\", \"no_of_weekend_nights\", \"no_of_week_nights\", \"type_of_meal_plan\", \"required_car_parking_space\", \"room_type_reserved\",\n",
    "\"arrival_year\", \"arrival_month\", \"arrival_date\", \"market_segment_type\", \"repeated_guest\", \"no_of_special_requests\", \"no_of_previous_cancellations\", \"no_of_previous_bookings_not_canceled\"]\n",
    "\n",
    "for feat in categorical_feats:\n",
    "    print(\"Unique values in \", feat, \" are: \", np.sort(train_df[feat].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of our features have integer data types, but some of these features are categorical or ordinal. Looking at the unique values within each feature can help us determine which category the data falls into. For example, room_type_reserved has an integer datatype but each integer represents a different room type as defined by INN Hotels so this data is actually categorical. Some of the integer features which take a small, finite set of values could also be considered ordinal. For example, no_of_adults only takes value from 0 to 4 and there is a clear ordered relation between consecutive values so we could treat this feature as ordinal instead of numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_cat = [\"no_of_previous_bookings_not_canceled\", \"no_of_previous_cancellations\"]\n",
    "\n",
    "num_bins = 10\n",
    "\n",
    "for col in previous_cat:\n",
    "    \n",
    "    axes = not_cancel[[col]].hist(bins = num_bins, alpha = 0.5, label=\"Cancelled\")[0][0]\n",
    "    #axes.hist(not_cancel[[col]], bins = num_bins, alpha = 0.5, label=\"Not-Cancel\")\n",
    "\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.title(\"Histogram of \" + col + \" by Cancellation\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "cancel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percent repeat guest:\", train_df.query(\"repeated_guest==1\").shape[0] / train_df.shape[0] * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the histograms of previous bookings cancelled versus not cancelled, we see that the vast majority of customers have not either cancelled or completed a previous reservation. Checking the repeated guests feature, we can see that only 2.5% of guests are repeat customers. Therefore it makes sense that there wouldn't be many examples where previous bookings were cancelled or not-cancelled. It could be useful to drop these features as they generally don't add much information, but they could be very useful in the few cases where they are not zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Observations\n",
    "\n",
    "From the exploratory data analysis, it is clear that there are many potential ways to categorize and process the data. For many of the features you could argue that they should considered numerical, ordinal, or categorical. For example, number of adults could be considered as simple numerical data, ordinal data since it is discrete and there is a clear relationship between each value (difference between 2 and 1 is one person), or even categorical if you don't think the relationship between different numbers of adults matters. There is also some ambiguity with the data in its current form. The type of meal plan or room type feature could be ordinal but it's impossible to conclude that without knowing more about the hotel's systems.  The number of special requests also is hard to interpret because not all special requests are the same and reducing them all to a count loses some specificity. \n",
    "\n",
    "Overall, it seems like it will be important to try different methods of pre-processing in order to ensure that the most accurate representation of the given data is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment Metrics\n",
    "\n",
    "Since the class imbalance is not extreme, accuracy will still be a good indicator of model performance. However, we think precision will be the most important metric to optimize. If the model predicts that a customer is likely to cancel their reservation, the hotel will likely try to re-book the room in preparation for a cancellation. If this prediction was a false positive and the customer does not cancel, the hotel will have a double-booked room which will be costly from both a customer satisfaction and economic standpoint. The customer won't be happy about not getting their desired room and the hotle will likely have to compensate the customer for the inconvenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) 4. Feature engineering <a name=\"4\"></a>\n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Carry out feature engineering. In other words, extract new features relevant for the problem and work with your new feature set in the following exercises. You may have to go back and forth between feature engineering and preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing and transformations <a name=\"5\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Identify different feature types and the transformations you would apply on each feature type. \n",
    "2. Define a column transformer, if necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric features: lead time, average room price, number previous cancellations, number previous bookings not cancelled\n",
    "* Transformations: Scaling\n",
    "* Notes: lead time, previous cancellations, and previous bookings all take integer values but can take a wide range of values so we decided to treat them as numeric and scale  \n",
    "\n",
    "Categorical features: type of meal plan, room type reserved, market segment type, arrival year, arrival month, arrival date\n",
    "* Transformations: One Hot Encoding\n",
    "\n",
    "Ordinal features: number adults, number children, number of weekend nights, number of week nights, number special requests\n",
    "* Transformations: None (Data is already in an ordinal form)\n",
    "* Notes: Chose ordinal for these feature because there is generally limited values that appear in data and there is a clear relationship between consecutive numbers within the data\n",
    "\n",
    "Binary features: repeated guest, required parking space\n",
    "* Transformations: None (Data already in binary form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = [\"lead_time\", \"avg_price_per_room\", \"no_of_previous_cancellations\", \"no_of_previous_bookings_not_canceled\"]\n",
    "categorical_feats = [\"type_of_meal_plan\", \"room_type_reserved\", \"market_segment_type\", \"arrival_year\", \"arrival_month\", \"arrival_date\"]\n",
    "pass_through = [\"repeated_guest\", \"required_car_parking_space\", \"no_of_adults\", \"no_of_children\", \"no_of_weekend_nights\", \"no_of_week_nights\", \"no_of_special_requests\"]\n",
    "target = [\"booking_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=target)\n",
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.drop(columns=target)\n",
    "y_train = train_df[target]\n",
    "y_test = test_df[target]\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric_feats),\n",
    "    (OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\"),  categorical_feats),\n",
    "    (\"passthrough\", pass_through)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for creating dataframe from transformed X modified from HW3 Implementation\n",
    "X_train_trans = preprocessor.fit_transform(X_train)\n",
    "print(X_train_trans.shape)\n",
    "\n",
    "column_names = (numeric_feats + preprocessor.named_transformers_[\"onehotencoder\"].get_feature_names_out().tolist() + pass_through)\n",
    "print(len(column_names))\n",
    "column_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans_df = pd.DataFrame(X_train_trans.toarray(), columns=column_names)\n",
    "X_train_trans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline model <a name=\"6\"></a>\n",
    "<hr>\n",
    "\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try `scikit-learn`'s baseline model and report results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From lecture 3\n",
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "    \"\"\"\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.4f (+/- %0.4f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "#Including f1 and recall just for reference even though they are not our key metrics\n",
    "scoring_metrics = [\"accuracy\",\"precision\", \"recall\", \"f1\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision, recall, and f1 score are ill-defined when using dummy classifier. By default sklearn sets these values to 0\n",
    "#Below import ignores the warning thrown\n",
    "import warnings\n",
    "from sklearn.metrics._classification import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "dummy_pipe = make_pipeline(preprocessor, DummyClassifier(strategy=\"most_frequent\"))\n",
    "\n",
    "results[\"Dummy Classifier\"] = mean_std_cross_val_scores(dummy_pipe, X_train, y_train, cv=10, scoring= scoring_metrics, return_train_score=True)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running $10$ fold cross validation on the dummy classifier yielded average test and validation score results of $0.67$. Precision is ill-defined for our dummy classifier because it always predicts the most common category (negative) therefore there are zero true and false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Linear models <a name=\"7\"></a>\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try logistic regression as a first real attempt. \n",
    "2. Carry out hyperparameter tuning to explore different values for the complexity hyperparameter `C`. \n",
    "3. Report validation scores along with standard deviation. \n",
    "4. Summarize your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))\n",
    "\n",
    "results[\"Default LogReg\"] = mean_std_cross_val_scores(lr_pipe, X_train, y_train.values.ravel(),scoring=scoring_metrics, return_train_score=True)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default hyper parameter C, the mean and standard deviation of the 10 fold cross validation scores are $\\mu_{val} \\approx 0.80$, $\\sigma_{val} \\approx 0.007$. These are surprisingly good results for a first attempt, so it is important to be diligent in verifying whether any over/under-fitting has occurred. \n",
    "\n",
    "A typical sign of under-fitting is similar, low train and validation score values. The average train score $\\mu_{train} \\approx 0.81$ and the average validation score presented above are fairly close. However, they are both high enough that we can fairly confidently rule out the presence of under-fitting phenomena. Further, we can say that this first attempt model is doing a good job of generalizing to unseen data and has suitable complexity for the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencing these baseline linear model results, we can perform hyper-parameter optimization/class-weight optimization to zero-in on a best linear model configuration. Since it is most important for our model to be precise in it's prediction of cancellations (so that the hotel can confidently take steps to account for the cancellation), we should choose \"precision\" as the scoring metric to be maximized. We also want to maintain a similar accuracy score of ~0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "param_dist = {\n",
    "    \"logisticregression__C\": uniform(0.01,10), #1e4\n",
    "    \"logisticregression__class_weight\": [None, {0:1, 1:3}, \"balanced\"],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    lr_pipe, param_dist, scoring=\"precision\", n_iter=100, verbose=1, n_jobs=-1, random_state=123\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best score: {s}\".format(s=random_search.best_score_))\n",
    "print(\"best C and class_weight: {c}\".format(c=random_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe_best = make_pipeline(preprocessor, LogisticRegression(max_iter=1000, C=random_search.best_params_[\"logisticregression__C\"], \n",
    "                                                              class_weight=random_search.best_params_[\"logisticregression__class_weight\"]))\n",
    "results[\"Best LogReg\"] = mean_std_cross_val_scores(lr_pipe_best, X_train, y_train.values.ravel(), scoring=scoring_metrics, return_train_score=True)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing original linear model to model with optimized hyper-parameter and class weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing hyper-parameter optimization using randomized search, it was determined that the best performing hyper-parameter value was $C = 0.2855$ and the best performing class weight was `None`. Using these optimized parameters yielded slight improvements in the average validation accuracy, and significant improvements in the average precision. The best average accuracy is 0.8062, the best average precision is 0.7367, and the standard deviation of validation accuracy scores is $\\sigma_{acc} = 0.00862$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Different classifiers <a name=\"8\"></a>\n",
    "<hr>\n",
    "rubric={points:15}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try at least 3 other models aside from logistic regression. At least one of these models should be a tree-based ensemble model (e.g., lgbm, random forest, xgboost). \n",
    "2. Summarize your results. Can you beat logistic regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipe = make_pipeline(preprocessor, SVC())\n",
    "results[\"Default SVM\"] = mean_std_cross_val_scores(svm_pipe, X_train, y_train.values.ravel(), scoring=scoring_metrics,return_train_score=True)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model achieves train and validation accuracy scores of 0.8547 and 0.8412 respectively. These scores are higher than what the linear model was able to achieve. The average precision scores achieved by the SVM are also higher than the linear model at 0.82 and 0.80 for train and validation respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_pipe = make_pipeline(preprocessor, KNeighborsClassifier())\n",
    "results[\"Default kNN\"] = mean_std_cross_val_scores(knn_pipe, X_train, y_train.values.ravel(), scoring=scoring_metrics,return_train_score=True)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "pipe_catboost = make_pipeline(\n",
    "    preprocessor, CatBoostClassifier(verbose=0, random_state=123)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"CatBoost\"] = mean_std_cross_val_scores(pipe_catboost, X_train, y_train, scoring=scoring_metrics, cv=10, return_train_score=True)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all models that have been tried so far it is clear that Catboost, our tree-based ensemble model, is performing the best on the given data set. The validation accuracy is almost 5% higher than the next closest model (SVM) and almost 9% better than default Logistic Regression. It also has the highest score in precision, recall, and f1 score, all by fairly sizeable margins. From a timing perspective, it is slower to train the model than Logistic Regression or kNN, but it is still over a second faster on average than the SVM. Additionally, it is very fast to predict, with it's validation time being comparable to that of the Logistic Regression model. The SVM is the next best performing model in our key metrics of accuracy and precision, but interestingly the kNN has a slightly higher recall score but lower accuracy and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) 9. Feature selection <a name=\"9\"></a>\n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to select relevant features. You may try `RFECV` or forward selection. Do the results improve with feature selection? Summarize your results. If you see improvements in the results, keep feature selection in your pipeline. If not, you may abandon it in the next exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter optimization <a name=\"10\"></a>\n",
    "<hr>\n",
    "rubric={points:15}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to optimize hyperparameters for the models you've tried and summarize your results. You may pick one of the best performing models from the previous exercise and tune hyperparameters only for that model. You may use `sklearn`'s methods for hyperparameter optimization or fancier Bayesian optimization methods. \n",
    "  - [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)   \n",
    "  - [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "  - [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM: gamma, C\n",
    "KNN: n_neighbours, C\n",
    "CatBoost: n_estimators, learning_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From part 8 we can see that the CatBoostClassifier performed best based on accuracy, precision, recall, and f1 scores. So we will tune the hyperparameters of this model. From documentation we know by default `n_estimators = 1000`, and the `learning_rate` is automatically assigned a value based on iterations. But we can pass our own values to tune these hyperparameters. In the code below, I have selected values above and below the default for `n_estimators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_cat = {\n",
    "    \"catboostclassifier__n_estimators\": [500,1000,2000,3000],\n",
    "    \"catboostclassifier__learning_rate\": np.logspace(-1,-0.4,5,endpoint=False),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the heatmap of our gridsearch below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe_catboost, param_grid_cat, cv=5, n_jobs=-1, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#adapted from lecture 8\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "scores = np.array(results.mean_test_score).reshape(param_grid_cat[\"catboostclassifier__n_estimators\"],3)\n",
    "scores\n",
    "import seaborn\n",
    "\n",
    "# plot the mean cross-validation scores\n",
    "seaborn.heatmap(\n",
    "    scores,\n",
    "    xticklabels=param_grid_cat[\"logisticregression__class_weight\"],\n",
    "    yticklabels=param_grid_cat[\"logisticregression__C\"],\n",
    "    cmap=\"viridis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "        pipe_catboost, param_distributions=param_grid_cat, n_jobs=-1, n_iter=10, cv=5, random_state=123\n",
    "    )\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = random_search.best_params_['catboostclassifier__n_estimators']\n",
    "learning_rate = random_search.best_params_['catboostclassifier__learning_rate']\n",
    "random_search.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results we can see the optimal results for `n_estimators` is around `1000~2000`, suggesting that the model is overfitted for more `n_estimators`. And we found or optimal `learning_rate` to be around `0.1~0.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interpretation and feature importances <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:15}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Use the methods we saw in class (e.g., `eli5`, `shap`) (or any other methods of your choice) to explain feature importances of one of the best performing models. Summarize your observations. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from lecture 11\n",
    "\n",
    "import shap\n",
    "\n",
    "pipe_catboost = make_pipeline(\n",
    "    preprocessor, CatBoostClassifier(verbose=0, random_state=123, learning_rate =learning_rate,n_estimators =  n_estimators)\n",
    ")\n",
    "\n",
    "pipe_catboost.fit(X_train,y_train)\n",
    "explainer = shap.TreeExplainer(pipe_catboost.named_steps[\"catboostclassifier\"])\n",
    "train_shap_values = explainer.shap_values(X_train_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.abs(train_shap_values).mean(0)\n",
    "pd.DataFrame(data=values, index=column_names, columns=[\"SHAP\"]).sort_values(\n",
    "    by=\"SHAP\", ascending=False\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(train_shap_values, X_train_trans_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SHAP values tell us lead time, no. special requests, and the average price per room have the most impact in the model. This is congruent with the eli5 values we will see in the next section. Looking more closer at the SHAP values for the top three features we see `positive correlation` to cancellation with `lead time` and `average room price`, and `negative correlation` to cancellation with `number of special requests`.\n",
    "\n",
    "- Clustering for `lead time` tells us there is a more frequest negative SHAP value that corresponds to low `lead time`. \n",
    "\n",
    "- Clustering for `no special requests` tells us there is a more frequest positive SHAP value that corresponds to low `no special requests`. \n",
    "\n",
    "- Clustering for `ave room price` tells us in many examples, room price is not the more impactful feature for predicting cancellation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELI5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "eli5.explain_weights(pipe_catboost.named_steps[\"catboostclassifier\"], feature_names=column_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Documentation](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html#:~:text=eli5%20provides%20a%20way%20to,Decrease%20Accuracy%20(MDA)%E2%80%9D.) tells us the `eli5` score is a prediction of how much the score may decrease when a feature is removed. As we can see above, the sum of the first three `eli5` scores is about `0.54`. Our best cv score from our hyperparameter search was `0.8895`. So, removing these three values could decrease our model accuracy to around `35%`. Since `32.6%` of total reservations are cancelled, removing these features would result in a model that doesn't perform much better than a Dummy Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results on the test set <a name=\"12\"></a>\n",
    "<hr>\n",
    "\n",
    "rubric={points:5}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try your best performing model on the test data and report test scores. \n",
    "2. Do the test scores agree with the validation scores from before? To what extent do you trust your results? Do you think you've had issues with optimization bias? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "y_pred = pipe_catboost.predict(X_test)\n",
    "\n",
    "print('accuracy = ', accuracy_score(y_test,y_pred))\n",
    "print('f1 = ', f1_score(y_test,y_pred))\n",
    "print('precision = ', precision_score(y_test,y_pred))\n",
    "print('recall = ', recall_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_catboost.score(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are very similar to what we found in question 8. Which makes sense since hyperparameter optimization found hyperparameter values very close to the defaults. The train score, however, is still significantly higher than the test score, suggesting our model is overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) 13. Explaining predictions \n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "1. Take one or two test predictions and explain them with SHAP force plots.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary of results <a name=\"13\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Report your final test score along with the metric you used. \n",
    "2. Write concluding remarks.\n",
    "3. Discuss other ideas that you did not try but could potentially improve the performance/interpretability . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit ('cpsc330': conda)",
   "name": "python3108jvsc74a57bd08915ad7360fbc0c1a6d7701a636ef4118c94c490cbdcb890ee3f668a7b9a7c85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a98c3d85c1abc903362c47fbead657c16687ad5ada0c916e3559fef74e209b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}